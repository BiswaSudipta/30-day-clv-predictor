{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " STEP 1: Load & Prepare Data"
      ],
      "metadata": {
        "id": "YrrGOn-qi4iV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"yasserh/instacart-online-grocery-basket-analysis-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjRBKsAEizan",
        "outputId": "a2a75aa7-c68e-44e9-b707-d4338bdfb8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'instacart-online-grocery-basket-analysis-dataset' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import lightgbm as lgb\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "XH4jEO_tjOsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f'{path}/aisles.csv')"
      ],
      "metadata": {
        "id": "R2srKGfkizzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "orders = pd.read_csv(f'{path}/orders.csv')\n",
        "prior = pd.read_csv(f'{path}/order_products__prior.csv')\n",
        "#train = pd.read_csv(f'{path}/order_products__train.csv')\n",
        "\n",
        "# Merge\n",
        "data = prior.merge(orders, on=\"order_id\", how=\"left\")\n",
        "\n",
        "# Sort for temporal logic\n",
        "data = data.sort_values([\"user_id\", \"order_number\"])\n"
      ],
      "metadata": {
        "id": "96LFJxVKjhCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Define Cutoff (LEAK-PROOF)"
      ],
      "metadata": {
        "id": "mQ4uVJl4i-Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CUTOFF_ORDER = 25\n",
        "\n",
        "past_data = data[data[\"order_number\"] <= CUTOFF_ORDER]\n",
        "future_data = data[\n",
        "    (data[\"order_number\"] > CUTOFF_ORDER) &\n",
        "    (data[\"order_number\"] <= CUTOFF_ORDER + 3)  # ~30 days in Instacart cycles\n",
        "]\n"
      ],
      "metadata": {
        "id": "GRuYfzQ1j-eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 3: Feature Engineering (PAST ONLY)"
      ],
      "metadata": {
        "id": "o8LOAoSIjAh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = past_data.groupby(\"user_id\").agg(\n",
        "    total_orders=(\"order_id\", \"nunique\"),\n",
        "    total_products=(\"product_id\", \"count\"),\n",
        "    reorder_rate=(\"reordered\", \"mean\"),\n",
        "    avg_days_between_orders=(\"days_since_prior_order\", \"mean\"),\n",
        "    recency_days=(\"days_since_prior_order\", \"last\"),\n",
        "    orders_last_5=(\"order_number\", lambda x: (x > CUTOFF_ORDER - 5).sum())\n",
        ").reset_index()\n"
      ],
      "metadata": {
        "id": "2yZHfdGikHXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 4: Target Creation (NEXT 30 DAYS)"
      ],
      "metadata": {
        "id": "VPgCarNFjCyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = future_data.groupby(\"user_id\").agg(\n",
        "    future_spend_30d=(\"product_id\", \"count\")\n",
        ").reset_index()\n",
        "\n",
        "dataset = features.merge(target, on=\"user_id\", how=\"left\")\n",
        "dataset[\"future_spend_30d\"] = dataset[\"future_spend_30d\"].fillna(0)\n"
      ],
      "metadata": {
        "id": "dYI2Ng-ekKam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Stage 1 — Buy / No Buy Model"
      ],
      "metadata": {
        "id": "z57QcZqMjFHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"will_buy_30d\"] = (dataset[\"future_spend_30d\"] > 0).astype(int)\n",
        "\n",
        "X = dataset.drop(columns=[\"user_id\", \"future_spend_30d\", \"will_buy_30d\"])\n",
        "y_class = dataset[\"will_buy_30d\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        ")\n",
        "\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "proba_test = clf.predict_proba(X_test)[:, 1]\n",
        "print(\"Stage 1 AUC:\", roc_auc_score(y_test, proba_test))\n"
      ],
      "metadata": {
        "id": "N5wq3FlekRYx",
        "outputId": "3804e2ab-1aa5-4d98-d0f0-cabeabff8252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 AUC: 0.9979030106826238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 6: Stage 2 — Spend Regression (ONLY BUYERS)"
      ],
      "metadata": {
        "id": "mEVIhNGajIeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buyers = dataset[dataset[\"future_spend_30d\"] > 0].copy()\n",
        "buyers[\"log_spend\"] = np.log1p(buyers[\"future_spend_30d\"])\n",
        "\n",
        "X_reg = buyers.drop(columns=[\"user_id\", \"future_spend_30d\", \"log_spend\", \"will_buy_30d\"])\n",
        "y_reg = buyers[\"log_spend\"]\n",
        "\n",
        "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "itopTkK4kT5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 1: Linear Regression (Baseline)"
      ],
      "metadata": {
        "id": "-_LgeFtWjLxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "lr.fit(Xr_train, yr_train)\n",
        "pred_lr = np.expm1(lr.predict(Xr_test))\n",
        "true_lr = np.expm1(yr_test)\n",
        "\n",
        "print(\"Linear MAE:\", mean_absolute_error(true_lr, pred_lr))\n"
      ],
      "metadata": {
        "id": "WqgT8xn1kWBE",
        "outputId": "cfde3006-3de7-4e30-f797-24b222f09375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear MAE: 10.217727325759661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 2: Random Forest Regressor"
      ],
      "metadata": {
        "id": "WUkg3UdXjOcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(Xr_train, yr_train)\n",
        "pred_rf = np.expm1(rf.predict(Xr_test))\n",
        "\n",
        "print(\"RandomForest MAE:\", mean_absolute_error(true_lr, pred_rf))\n"
      ],
      "metadata": {
        "id": "YhvvU5TFkXpq",
        "outputId": "1889887b-8d21-47ce-f424-d3c19e35197c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest MAE: 8.490630678292819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL 3: LightGBM (BEST MODEL)"
      ],
      "metadata": {
        "id": "_Nub0JM8jQXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgb_model.fit(Xr_train, yr_train)\n",
        "pred_lgb = np.expm1(lgb_model.predict(Xr_test))\n",
        "\n",
        "print(\"LightGBM MAE:\", mean_absolute_error(true_lr, pred_lgb))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "seuhW0JxkZiX",
        "outputId": "b80eb9d5-0ba3-4ca9-b2d9-fd7b8f237b21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001936 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 993\n",
            "[LightGBM] [Info] Number of data points in the train set: 28772, number of used features: 5\n",
            "[LightGBM] [Info] Start training from score 3.202597\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "LightGBM MAE: 8.509700508220291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 7: Final CLV Prediction Logic"
      ],
      "metadata": {
        "id": "9nSKlHT4jTs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final prediction = P(buy) × Expected spend\n",
        "\n",
        "test_users = X_test.copy()\n",
        "\n",
        "buy_prob = clf.predict_proba(test_users)[:, 1]\n",
        "expected_spend = np.expm1(lgb_model.predict(test_users))\n",
        "\n",
        "final_clv = buy_prob * expected_spend\n"
      ],
      "metadata": {
        "id": "U99ioQtskjJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " RMSE & R²"
      ],
      "metadata": {
        "id": "sHp6xOzcjhxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "HdLJcZ4aklTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_lgb = np.sqrt(mean_squared_error(true_lr, pred_lgb))\n",
        "r2_lgb = r2_score(true_lr, pred_lgb)\n",
        "\n",
        "print(\"LightGBM RMSE:\", rmse_lgb)\n",
        "print(\"LightGBM R2  :\", r2_lgb)\n"
      ],
      "metadata": {
        "id": "_AH-yxGmk36G",
        "outputId": "c5f116ac-9095-413a-f334-8f35f042df88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightGBM RMSE: 12.116639199355193\n",
            "LightGBM R2  : 0.6127120741297154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_rf = np.sqrt(mean_squared_error(true_lr, pred_rf))\n",
        "r2_rf = r2_score(true_lr, pred_rf)\n",
        "\n",
        "print(\"RandomForest RMSE:\", rmse_rf)\n",
        "print(\"RandomForest R2  :\", r2_rf)\n"
      ],
      "metadata": {
        "id": "zAAl6pY-k5r5",
        "outputId": "e6429d87-2a24-4bfb-b0d3-f28a1300700d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForest RMSE: 12.108283967036447\n",
            "RandomForest R2  : 0.6132460117775613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " STEP 8: Save Models (For FastAPI)"
      ],
      "metadata": {
        "id": "ZE0yfShJjZag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(clf, \"buy_probability_model.pkl\")\n",
        "joblib.dump(lgb_model, \"spend_regression_model.pkl\")\n",
        "joblib.dump(X.columns.tolist(), \"feature_columns.pkl\")"
      ],
      "metadata": {
        "id": "9l5SJDnmk9v2",
        "outputId": "79a291c4-d2d5-40a5-ebef-260f15e301e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feature_columns.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ===============================\n",
        "# 1. LOAD MODELS\n",
        "# ===============================\n",
        "\n",
        "buy_model = joblib.load(\"buy_probability_model.pkl\")\n",
        "spend_model = joblib.load(\"spend_regression_model.pkl\")\n",
        "\n",
        "# ===============================\n",
        "# 2. FEATURE ORDER (KNOWN)\n",
        "# ===============================\n",
        "\n",
        "FEATURE_ORDER = [\n",
        "    \"total_orders\",\n",
        "    \"total_products\",\n",
        "    \"reorder_rate\",\n",
        "    \"avg_days_between_orders\",\n",
        "    \"recency_days\",\n",
        "    \"orders_last_5\"\n",
        "]\n",
        "\n",
        "# ===============================\n",
        "# 3. PREDICTION FUNCTION\n",
        "# ===============================\n",
        "\n",
        "def predict_30d_clv(input_features: dict):\n",
        "    \"\"\"\n",
        "    input_features: dict with keys exactly matching FEATURE_ORDER\n",
        "    returns: predicted 30-day CLV\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert input dict → DataFrame\n",
        "    X = pd.DataFrame([input_features])[FEATURE_ORDER]\n",
        "\n",
        "    # Stage 1: Buy probability\n",
        "    buy_prob = buy_model.predict_proba(X)[:, 1][0]\n",
        "\n",
        "    # Stage 2: Expected spend (log scale → original)\n",
        "    expected_spend = np.expm1(spend_model.predict(X))[0]\n",
        "\n",
        "    # Final CLV\n",
        "    final_clv = buy_prob * expected_spend\n",
        "\n",
        "    return {\n",
        "        \"buy_probability\": round(float(buy_prob), 4),\n",
        "        \"expected_spend_if_buy\": round(float(expected_spend), 2),\n",
        "        \"predicted_30d_clv\": round(float(final_clv), 2)\n",
        "    }\n",
        "\n",
        "# ===============================\n",
        "# 4. EXAMPLE USAGE\n",
        "# ===============================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    sample_input = {\n",
        "        \"total_orders\": 30,\n",
        "        \"total_products\": 180,\n",
        "        \"reorder_rate\": 0.65,\n",
        "        \"avg_days_between_orders\": 7.2,\n",
        "        \"recency_days\": 3,\n",
        "        \"orders_last_5\": 4\n",
        "    }\n",
        "\n",
        "    prediction = predict_30d_clv(sample_input)\n",
        "    print(prediction)\n"
      ],
      "metadata": {
        "id": "iG1Z6b7HmGQh",
        "outputId": "50b6a340-384d-4806-b8ba-566d923bcb57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'buy_probability': 1.0, 'expected_spend_if_buy': 8.96, 'predicted_30d_clv': 8.96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4OVRokkumIzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}